{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ddcreating/RL_code/blob/main/lab4_dyna.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 4: TD and Dyna\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3O73YRqcSpVN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1: Implement SARSA with n-step TD (n=5) on CliffWalking\n",
        "\n",
        "**Objective:**  \n",
        "In this exercise, you will implement the **SARSA algorithm** using **n-step temporal-difference learning with n=5**. You will apply your implementation to the **CliffWalking environment** in Gymnasium, and analyze how multi-step returns influence learning compared to standard 1-step SARSA.\n",
        "\n",
        "---\n",
        "\n",
        "### Environment\n",
        "- Use `CliffWalking-v1`\n",
        "\n",
        "---\n",
        "\n",
        "### Instructions\n",
        "1. Implement **SARSA with n-step TD updates (n=5)**:\n",
        "   - Maintain an action-value table \\(Q(s,a)\\).\n",
        "   - Use ε-greedy exploration.\n",
        "   - Store states, actions, and rewards for the last 5 steps.\n",
        "   - After each step, compute the n-step return: G_t\n",
        "   - Update \\(Q(s_t,a_t)\\) toward \\(G_t\\).\n",
        "\n",
        "2. Train your agent for several thousand episodes (e.g., 5,000).\n",
        "\n",
        "3. Plot the **episode rewards over time** to visualize learning progress.\n",
        "\n",
        "4. Compare qualitatively with 1-step SARSA:\n",
        "   - Does n-step SARSA converge faster or slower?\n",
        "   - How do the policies differ near the cliff?\n",
        "\n",
        "---\n",
        "\n",
        "### Deliverables\n",
        "- Python code implementing SARSA with TD(5) (notebook in Github).  \n",
        "- A plot of episode number vs episode return (plot in a cell below).  \n",
        "- A short discussion (1 paragraph) comparing the results with standard SARSA.  \n"
      ],
      "metadata": {
        "id": "mnJD-ntoxjeR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ORsNHBnkSbyS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "c0a4de1d-f099-4cc3-b5cb-3d7d4254d357"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "deque index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3989921132.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0mG\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtau\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtau\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn_step\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0ms_tau_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtau\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m                 \u001b[0ma_tau_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtau\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0mG\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mn_step\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms_tau_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_tau_n\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: deque index out of range"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Starter code for Exercise (you can use this code, or extend your code from previous lab)\n",
        "Implement SARSA with TD(5) on CliffWalking-v1\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Environment\n",
        "env = gym.make(\"CliffWalking-v1\")\n",
        "\n",
        "# Parameters\n",
        "n_states = env.observation_space.n\n",
        "n_actions = env.action_space.n\n",
        "alpha = 0.1           # step size (learning rate)\n",
        "gamma = 0.99          # discount factor\n",
        "epsilon = 0.1         # epsilon for epsilon-greedy policy\n",
        "n_step = 5            # number of steps for TD(n)\n",
        "n_episodes = 5000\n",
        "\n",
        "# Initialize Q-table\n",
        "Q = np.zeros((n_states, n_actions))\n",
        "\n",
        "def epsilon_greedy(state):\n",
        "    \"\"\"Choose an action using epsilon-greedy policy.\"\"\"\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.randint(n_actions)\n",
        "    return np.argmax(Q[state])\n",
        "\n",
        "# Track returns\n",
        "episode_returns = []\n",
        "\n",
        "for ep in range(n_episodes):\n",
        "    state, _ = env.reset()\n",
        "    action = epsilon_greedy(state)\n",
        "\n",
        "    # Buffers to store the trajectory\n",
        "    states = deque()\n",
        "    actions = deque()\n",
        "    rewards = deque()\n",
        "\n",
        "    T = float(\"inf\")\n",
        "    t = 0\n",
        "    G = 0\n",
        "    done = False\n",
        "\n",
        "    while True:\n",
        "        if t < T:\n",
        "            # Take real step in the environment\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            states.append(state)\n",
        "            actions.append(action)\n",
        "            rewards.append(reward)\n",
        "\n",
        "            if done:\n",
        "                T = t + 1\n",
        "            else:\n",
        "                next_action = epsilon_greedy(next_state)\n",
        "                state = next_state\n",
        "                action = next_action\n",
        "\n",
        "        # Time index for state/action to update\n",
        "        tau = t - n_step + 1\n",
        "        if tau >= 0:\n",
        "            # TODO: Compute the n-step return G for state tau\n",
        "            # Hint: use rewards[tau : tau+n] plus Q(s_t+n, a_t+n) if not terminal\n",
        "\n",
        "            # Example structure:\n",
        "            G = 0.0\n",
        "            # accumulate discounted rewards\n",
        "            for i in range(tau, min(tau + n_step, T)):\n",
        "                G += (gamma ** (i - tau)) * rewards[i]\n",
        "            if tau + n_step < T:\n",
        "                s_tau_n = states[tau + n_step]\n",
        "                a_tau_n = actions[tau + n_step]\n",
        "                G += (gamma ** n_step) * Q[s_tau_n, a_tau_n]\n",
        "\n",
        "            # TODO: Update Q[states[tau], actions[tau]] toward G\n",
        "            Q[states[tau], actions[tau]] += alpha * (G - Q[states[tau], actions[tau]])\n",
        "\n",
        "        if tau == T - 1:\n",
        "            break\n",
        "\n",
        "        t += 1\n",
        "\n",
        "    episode_returns.append(sum(rewards))\n",
        "\n",
        "# Plot learning curve\n",
        "plt.plot(episode_returns)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Return\")\n",
        "plt.title(\"SARSA with TD(5) on CliffWalking\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np, math, random\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def set_seed(env, seed=0):\n",
        "    np.random.seed(seed); random.seed(seed)\n",
        "    try:\n",
        "        env.reset(seed=seed)\n",
        "        env.action_space.seed(seed)\n",
        "        env.observation_space.seed(seed)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "def epsilon_greedy(Q, s, nA, eps):\n",
        "    if np.random.rand() < eps: return np.random.randint(nA)\n",
        "    qs = Q[s]; mx = qs.max()\n",
        "    return np.random.choice(np.flatnonzero(qs == mx))\n",
        "\n",
        "def moving_avg(bools, k=100):\n",
        "    out, q = [], deque(maxlen=k)\n",
        "    for v in bools:\n",
        "        q.append(1 if v else 0)\n",
        "        out.append(sum(q)/len(q))\n",
        "    return out\n",
        "\n",
        "def smooth(x, k=200):\n",
        "    out, q = [], deque(maxlen=k)\n",
        "    for v in x:\n",
        "        q.append(v); out.append(sum(q)/len(q))\n",
        "    return out\n",
        "\n",
        "# ---------- SARSA(n) ----------\n",
        "def sarsa_n_step(env, n_step=5, episodes=5000, alpha=0.1, gamma=0.99,\n",
        "                 eps_start=0.1, eps_end=0.02, eps_decay=0.999):\n",
        "    import math, numpy as np\n",
        "    from collections import deque\n",
        "\n",
        "    nS, nA = env.observation_space.n, env.action_space.n\n",
        "    Q = np.zeros((nS, nA))\n",
        "    returns, successes = [], []\n",
        "\n",
        "    def eps_greedy(Q, s, nA, eps):\n",
        "        if np.random.rand() < eps: return np.random.randint(nA)\n",
        "        qs = Q[s]; mx = qs.max()\n",
        "        return np.random.choice(np.flatnonzero(qs == mx))\n",
        "\n",
        "    eps = eps_start\n",
        "    for _ in range(episodes):\n",
        "        s, _ = env.reset()\n",
        "        a = eps_greedy(Q, s, nA, eps)\n",
        "\n",
        "        states, actions, rewards = [], [], []\n",
        "        T = math.inf  # 还没终止前保持无穷\n",
        "        t = 0\n",
        "        ep_ret = 0.0\n",
        "\n",
        "        while True:\n",
        "            if t < T:\n",
        "                s2, r, terminated, truncated, _ = env.step(a)\n",
        "                done = terminated or truncated\n",
        "\n",
        "                states.append(s)\n",
        "                actions.append(a)\n",
        "                rewards.append(r)\n",
        "                ep_ret += r\n",
        "\n",
        "                if done:\n",
        "                    T = t + 1           # 首次终止时确定真实 T\n",
        "                else:\n",
        "                    a2 = eps_greedy(Q, s2, nA, eps)\n",
        "                    s, a = s2, a2\n",
        "\n",
        "            tau = t - n_step + 1\n",
        "            if tau >= 0:\n",
        "                # --------- 计算 n-step return ----------\n",
        "                # 累积奖励：i 从 tau 到 min(tau+n-1, T-1)\n",
        "                if math.isinf(T):\n",
        "                    last_i = min(tau + n_step - 1, len(rewards) - 1)\n",
        "                else:\n",
        "                    last_i = min(tau + n_step - 1, int(T) - 1)\n",
        "\n",
        "                G = 0.0\n",
        "                for i in range(tau, last_i + 1):\n",
        "                    G += (gamma ** (i - tau)) * rewards[i]\n",
        "\n",
        "                # 自举项：只有当 (tau+n) < T 且缓冲里确有该索引时才加入\n",
        "                if (not math.isinf(T)) and (tau + n_step < T) and (tau + n_step < len(states)):\n",
        "                    G += (gamma ** n_step) * Q[states[tau + n_step], actions[tau + n_step]]\n",
        "\n",
        "                Q[states[tau], actions[tau]] += alpha * (G - Q[states[tau], actions[tau]])\n",
        "                # -------------------------------------\n",
        "\n",
        "            if tau == T - 1:\n",
        "                break\n",
        "            t += 1\n",
        "\n",
        "        returns.append(ep_ret)\n",
        "        successes.append(0 in rewards)   # 到达终点那步 reward=0\n",
        "        eps = max(eps_end, eps * eps_decay)\n",
        "\n",
        "    return Q, returns, successes\n",
        "\n",
        "\n",
        "# ---------- SARSA(1) ----------\n",
        "def sarsa_1_step(env, episodes=5000, alpha=0.1, gamma=0.99,\n",
        "                 eps_start=0.1, eps_end=0.02, eps_decay=0.999):\n",
        "    nS, nA = env.observation_space.n, env.action_space.n\n",
        "    Q = np.zeros((nS, nA))\n",
        "    returns, successes = [], []\n",
        "\n",
        "    eps = eps_start\n",
        "    for _ in range(episodes):\n",
        "        s, _ = env.reset()\n",
        "        a = epsilon_greedy(Q, s, nA, eps)\n",
        "        done, ep_return = False, 0.0\n",
        "        rewards = []\n",
        "        while not done:\n",
        "            s2, r, terminated, truncated, _ = env.step(a)\n",
        "            done = terminated or truncated\n",
        "            rewards.append(r); ep_return += r\n",
        "            td_target = r if done else r + gamma * Q[s2, epsilon_greedy(Q, s2, nA, eps)]\n",
        "            Q[s, a] += alpha * (td_target - Q[s, a])\n",
        "            s = s2\n",
        "            if not done:\n",
        "                a = epsilon_greedy(Q, s, nA, eps)\n",
        "\n",
        "        returns.append(ep_return)\n",
        "        successes.append(0 in rewards)\n",
        "        eps = max(eps_end, eps * eps_decay)\n",
        "\n",
        "    return Q, returns, successes\n",
        "\n",
        "# ---------- run ----------\n",
        "EPISODES = 5000\n",
        "ALPHA, GAMMA = 0.1, 0.99\n",
        "EPS0, EPSF, EDECAY = 0.1, 0.02, 0.999\n",
        "\n",
        "# 关键：限制单局最长步数，避免返回值失控\n",
        "env1 = gym.make(\"CliffWalking-v1\", max_episode_steps=500); set_seed(env1, 0)\n",
        "Q5, ret5, suc5 = sarsa_n_step(env1, n_step=5, episodes=EPISODES, alpha=ALPHA, gamma=GAMMA,\n",
        "                              eps_start=EPS0, eps_end=EPSF, eps_decay=EDECAY)\n",
        "env2 = gym.make(\"CliffWalking-v1\", max_episode_steps=500); set_seed(env2, 0)\n",
        "Q1, ret1, suc1 = sarsa_1_step(env2, episodes=EPISODES, alpha=ALPHA, gamma=GAMMA,\n",
        "                              eps_start=EPS0, eps_end=EPSF, eps_decay=EDECAY)\n",
        "\n",
        "# ---------- plots ----------\n",
        "# (1) 前 500 局放大\n",
        "plt.figure(figsize=(8,4))\n",
        "x = np.arange(500)\n",
        "plt.plot(x, ret1[:500], label=\"1-step\")\n",
        "plt.plot(x, ret5[:500], label=\"n=5\")\n",
        "plt.xlabel(\"Episode (first 500)\"); plt.ylabel(\"Return\")\n",
        "plt.title(\"Early Learning (CliffWalking)\")\n",
        "plt.legend(); plt.tight_layout(); plt.show()\n",
        "\n",
        "# (2) 成功率（avg100）\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.plot(moving_avg(suc1, 100), label=\"1-step success (avg100)\")\n",
        "plt.plot(moving_avg(suc5, 100), label=\"n=5 success (avg100)\")\n",
        "plt.xlabel(\"Episode\"); plt.ylabel(\"Success rate\")\n",
        "plt.title(\"Success Rate Over Time\"); plt.legend(); plt.tight_layout(); plt.show()\n",
        "\n",
        "# (3) 全局回报 + avg200\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.plot(ret1, label=\"1-step (raw)\")\n",
        "plt.plot(ret5, label=\"n=5 (raw)\")\n",
        "plt.plot(smooth(ret1, 200), linestyle=\"--\", label=\"1-step (avg200)\")\n",
        "plt.plot(smooth(ret5, 200), linestyle=\"--\", label=\"n=5 (avg200)\")\n",
        "plt.xlabel(\"Episode\"); plt.ylabel(\"Return\")\n",
        "plt.title(\"Returns Over Episodes\"); plt.legend(); plt.tight_layout(); plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "So6pIJ09Az4I",
        "outputId": "feb8058f-2469-4fa1-b3cb-16eacfcaca9a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OverflowError",
          "evalue": "cannot convert float infinity to integer",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3459385390.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;31m# 关键：限制单局最长步数，避免返回值失控\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0menv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CliffWalking-v1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_episode_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mset_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m Q5, ret5, suc5 = sarsa_n_step(env1, n_step=5, episodes=EPISODES, alpha=ALPHA, gamma=GAMMA,\n\u001b[0m\u001b[1;32m    120\u001b[0m                               eps_start=EPS0, eps_end=EPSF, eps_decay=EDECAY)\n\u001b[1;32m    121\u001b[0m \u001b[0menv2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CliffWalking-v1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_episode_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mset_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3459385390.py\u001b[0m in \u001b[0;36msarsa_n_step\u001b[0;34m(env, n_step, episodes, alpha, gamma, eps_start, eps_end, eps_decay)\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0;31m# n-step return\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtau\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtau\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                     \u001b[0mG\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtau\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOverflowError\u001b[0m: cannot convert float infinity to integer"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2: Dyna-Q for CliffWalking\n",
        "\n",
        "**Objective**  \n",
        "Implement **Dyna-Q** on **CliffWalking-v1** and compare its learning performance to **SARSA (1-step)** and **SARSA TD(5)**. You will analyze sample efficiency, stability near the cliff, and sensitivity to planning steps.\n",
        "\n",
        "---\n",
        "\n",
        "### Environment\n",
        "- Use `CliffWalking-v1`\n",
        "---\n",
        "\n",
        "### Part A — Dyna-Q (Implementation)\n",
        "1. **Q-table**: maintain `Q[s, a]` (tabular).\n",
        "2. **Model**: learn an empirical model from experience.\n",
        "   - For each observed transition `(s, a, r, s')`, update a dictionary:\n",
        "     - Minimal: store the most recent `(s', r)` for `(s, a)`, **or**\n",
        "     - Advanced: store a **multiset** of outcomes for `(s, a)` with counts (to sample stochastically).\n",
        "3. **Real update (Q-learning)** after each env step:\n",
        "   Q(s,a) ← Q(s,a) + α * (r + γ * max_a' Q(s',a') - Q(s,a))\n",
        "4. **Planning updates**: after each real step, perform `N` simulated updates:\n",
        "   - Sample a previously seen `(s_p, a_p)` from the model.\n",
        "   - Sample `(r_p, s'_p)` from that entry.\n",
        "   - Apply the same Q-learning backup using `(s_p, a_p, r_p, s'_p)`.\n",
        "5. Use epsilon-greedy exploration.\n",
        "\n",
        "---\n",
        "\n",
        "### Part B — Baselines (Re-use / Implement)\n",
        "- **SARSA (1-step)** with ε-greedy:\n",
        "  \\[\n",
        "  Q(s,a) \\leftarrow Q(s,a) + \\alpha\\big[r + \\gamma Q(s',a') - Q(s,a)\\big]\n",
        "  \\]\n",
        "- **SARSA TD(5)** (n-step SARSA with \\(n=5\\)), as in Exercise 1.\n",
        "\n",
        "Use the **same** γ, α, ε schedule, and number of episodes for a fair comparison.\n",
        "\n",
        "---\n",
        "\n",
        "### Part C — Experiments & Comparisons\n",
        "1. **Learning curves**: plot **episode index vs. episode return** for:\n",
        "   - Dyna-Q with \\(N \\in \\{5, 20, 50\\}\\)\n",
        "   - SARSA (1-step)\n",
        "   - SARSA TD(5)\n",
        "2. **Sample efficiency**: report the **episode number** at which the average return over a sliding window (e.g., 100 episodes) first exceeds a chosen threshold (e.g., −30).\n",
        "3. **Stability near the cliff**: qualitatively inspect trajectories/policies; does the method hug the cliff or leave a safer margin?\n",
        "4. **Sensitivity to planning steps**: compare Dyna-Q across N; discuss diminishing returns vs. computation.\n",
        "5. **Statistical robustness**: run **≥5 seeds**; plot mean ± std (shaded) or report mean ± std of final returns.\n",
        "\n",
        "---\n",
        "\n",
        "### Deliverables\n",
        "- **Code**: A driver script/notebook that reproduces your plots\n",
        "- **Plots** (embedded in the notebook):\n",
        "  - Learning curves (mean ± std across seeds)\n",
        "  - Optional: heatmap of greedy policy/actions on the grid\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "h8NKZuvP5GZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BhimOYaY0zZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 3: Solve FrozenLake with Q-Learning and Dyna-Q (Stochastic Model)\n",
        "\n",
        "**Objective**  \n",
        "Implement and compare **Q-learning** and **Dyna-Q** on Gymnasium’s `FrozenLake-v1`.  \n",
        "For Dyna-Q, your learned **transition model must handle multiple possible next states** per `(s, a)` (stochastic slip), i.e., store and sample **a distribution** over `(s', r)` outcomes rather than a single next state.\n",
        "\n",
        "---\n",
        "\n",
        "### Environment\n",
        "- Use `FrozenLake-v1` from `gymnasium.envs.toy_text`.\n",
        "- You can start with map 4×4; and then work with 8×8.\n",
        "- Start → Goal with slippery transitions (stochastic).  \n",
        "- Rewards: `+1` at goal, `0` otherwise (holes terminate with 0).\n",
        "\n",
        "---\n",
        "\n",
        "### Part A — Q-learning (baseline)\n",
        "1. Maintain a tabular action-value function `Q[s, a]`.\n",
        "2. Behavior: ε-greedy over `Q`.\n",
        "3. Update after each real step:\n",
        "   - target = r + γ * max_a' Q[s', a']   (if terminal: target = r)\n",
        "   - Q[s, a] ← Q[s, a] + α * (target − Q[s, a])\n",
        "4. Train for several thousand episodes (e.g., 5,000) with an ε schedule (e.g., 0.2 → 0.01).\n",
        "\n",
        "---\n",
        "\n",
        "### Part B — Dyna-Q with a **stochastic transition model**\n",
        "1. **Empirical model (multinomial):** for each `(s, a)`, maintain a multiset of observed outcomes:\n",
        "   - `model[(s, a)] = [(s'_1, r_1, count_1), (s'_2, r_2, count_2), ...]`\n",
        "   - Update counts whenever you observe `(s, a, r, s')`.\n",
        "2. **Real step update (Q-learning):** same as Part A.\n",
        "3. **Planning steps (N per real step):**\n",
        "   - Sample a previously seen `(s_p, a_p)` uniformly (or with priority).\n",
        "   - Sample `(s'_p, r_p)` **from the empirical distribution** for `(s_p, a_p)` using counts as probabilities.\n",
        "   - Apply the same Q-learning backup with `(s_p, a_p, r_p, s'_p)`.\n",
        "4. Train with the same ε schedule and number of episodes; vary `N ∈ {5, 20, 50}`.\n",
        "\n",
        "---\n",
        "\n",
        "### Experiments & Analysis\n",
        "1. **Learning curves:** plot episode index vs episode return (smoothed) for:\n",
        "   - Q-learning\n",
        "   - Dyna-Q (N=5, 20, 50)\n",
        "2. **Sample efficiency:** report the episode at which the moving-average return (e.g., window 100) first exceeds a threshold (you choose a reasonable value).\n",
        "3. **Effect of stochastic modeling:** briefly explain why storing a distribution over `(s', r)` matters on FrozenLake (slip), and what happens if you store only the most recent outcome.\n",
        "4. **Robustness:** run ≥5 random seeds; report mean ± std of final evaluation returns.\n",
        "\n",
        "---\n",
        "\n",
        "### Deliverables\n",
        "- **Code** for Q-learning and Dyna-Q (with stochastic model).  \n",
        "- **Plots** of learning curves (include legend and axis labels).  \n",
        "- ** Discussion:** why Dyna-Q helps here; impact of N; importance of modeling multiple next states.\n",
        "\n",
        "---\n",
        "\n",
        "### Hints\n",
        "- For terminal transitions (goal/hole), the Q-learning target is simply `target = r` (no bootstrap).  \n",
        "- When sampling from the model, use probabilities `p_i = count_i / sum_j count_j`.  \n",
        "- Tie-break greedy action selection uniformly among argmax actions to avoid bias.  \n",
        "- Keep evaluation **greedy (ε=0)** and consistent across methods (same seeds and episode counts).\n"
      ],
      "metadata": {
        "id": "E4iLQFaGzJLj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d7FHlfk700lr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}