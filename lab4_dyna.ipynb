{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ddcreating/RL_code/blob/main/lab4_dyna.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 4: TD and Dyna\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3O73YRqcSpVN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1: Implement SARSA with n-step TD (n=5) on CliffWalking\n",
        "\n",
        "**Objective:**  \n",
        "In this exercise, you will implement the **SARSA algorithm** using **n-step temporal-difference learning with n=5**. You will apply your implementation to the **CliffWalking environment** in Gymnasium, and analyze how multi-step returns influence learning compared to standard 1-step SARSA.\n",
        "\n",
        "---\n",
        "\n",
        "### Environment\n",
        "- Use `CliffWalking-v1`\n",
        "\n",
        "---\n",
        "\n",
        "### Instructions\n",
        "1. Implement **SARSA with n-step TD updates (n=5)**:\n",
        "   - Maintain an action-value table \\(Q(s,a)\\).\n",
        "   - Use Îµ-greedy exploration.\n",
        "   - Store states, actions, and rewards for the last 5 steps.\n",
        "   - After each step, compute the n-step return: G_t\n",
        "   - Update \\(Q(s_t,a_t)\\) toward \\(G_t\\).\n",
        "\n",
        "2. Train your agent for several thousand episodes (e.g., 5,000).\n",
        "\n",
        "3. Plot the **episode rewards over time** to visualize learning progress.\n",
        "\n",
        "4. Compare qualitatively with 1-step SARSA:\n",
        "   - Does n-step SARSA converge faster or slower?\n",
        "   - How do the policies differ near the cliff?\n",
        "\n",
        "---\n",
        "\n",
        "### Deliverables\n",
        "- Python code implementing SARSA with TD(5) (notebook in Github).  \n",
        "- A plot of episode number vs episode return (plot in a cell below).  \n",
        "- A short discussion (1 paragraph) comparing the results with standard SARSA.  \n"
      ],
      "metadata": {
        "id": "mnJD-ntoxjeR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ORsNHBnkSbyS",
        "outputId": "c0a4de1d-f099-4cc3-b5cb-3d7d4254d357",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "deque index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3989921132.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0mG\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtau\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtau\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn_step\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0ms_tau_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtau\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m                 \u001b[0ma_tau_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtau\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0mG\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mn_step\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms_tau_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_tau_n\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: deque index out of range"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Starter code for Exercise (you can use this code, or extend your code from previous lab)\n",
        "Implement SARSA with TD(5) on CliffWalking-v1\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Environment\n",
        "env = gym.make(\"CliffWalking-v1\")\n",
        "\n",
        "# Parameters\n",
        "n_states = env.observation_space.n\n",
        "n_actions = env.action_space.n\n",
        "alpha = 0.1           # step size (learning rate)\n",
        "gamma = 0.99          # discount factor\n",
        "epsilon = 0.1         # epsilon for epsilon-greedy policy\n",
        "n_step = 5            # number of steps for TD(n)\n",
        "n_episodes = 5000\n",
        "\n",
        "# Initialize Q-table\n",
        "Q = np.zeros((n_states, n_actions))\n",
        "\n",
        "def epsilon_greedy(state):\n",
        "    \"\"\"Choose an action using epsilon-greedy policy.\"\"\"\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.randint(n_actions)\n",
        "    return np.argmax(Q[state])\n",
        "\n",
        "# Track returns\n",
        "episode_returns = []\n",
        "\n",
        "for ep in range(n_episodes):\n",
        "    state, _ = env.reset()\n",
        "    action = epsilon_greedy(state)\n",
        "\n",
        "    # Buffers to store the trajectory\n",
        "    states = deque()\n",
        "    actions = deque()\n",
        "    rewards = deque()\n",
        "\n",
        "    T = float(\"inf\")\n",
        "    t = 0\n",
        "    G = 0\n",
        "    done = False\n",
        "\n",
        "    while True:\n",
        "        if t < T:\n",
        "            # Take real step in the environment\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            states.append(state)\n",
        "            actions.append(action)\n",
        "            rewards.append(reward)\n",
        "\n",
        "            if done:\n",
        "                T = t + 1\n",
        "            else:\n",
        "                next_action = epsilon_greedy(next_state)\n",
        "                state = next_state\n",
        "                action = next_action\n",
        "\n",
        "        # Time index for state/action to update\n",
        "        tau = t - n_step + 1\n",
        "        if tau >= 0:\n",
        "            # TODO: Compute the n-step return G for state tau\n",
        "            # Hint: use rewards[tau : tau+n] plus Q(s_t+n, a_t+n) if not terminal\n",
        "\n",
        "            # Example structure:\n",
        "            G = 0.0\n",
        "            # accumulate discounted rewards\n",
        "            for i in range(tau, min(tau + n_step, T)):\n",
        "                G += (gamma ** (i - tau)) * rewards[i]\n",
        "            if tau + n_step < T:\n",
        "                s_tau_n = states[tau + n_step]\n",
        "                a_tau_n = actions[tau + n_step]\n",
        "                G += (gamma ** n_step) * Q[s_tau_n, a_tau_n]\n",
        "\n",
        "            # TODO: Update Q[states[tau], actions[tau]] toward G\n",
        "            Q[states[tau], actions[tau]] += alpha * (G - Q[states[tau], actions[tau]])\n",
        "\n",
        "        if tau == T - 1:\n",
        "            break\n",
        "\n",
        "        t += 1\n",
        "\n",
        "    episode_returns.append(sum(rewards))\n",
        "\n",
        "# Plot learning curve\n",
        "plt.plot(episode_returns)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Return\")\n",
        "plt.title(\"SARSA with TD(5) on CliffWalking\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Exercise 1: SARSA with TD(5) on CliffWalking-v1 + 1-step baseline\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import random\n",
        "\n",
        "# -------------------- Utils --------------------\n",
        "def set_seed(env, seed=0):\n",
        "    np.random.seed(seed); random.seed(seed)\n",
        "    try:\n",
        "        env.reset(seed=seed)\n",
        "        env.action_space.seed(seed)\n",
        "        env.observation_space.seed(seed)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "def epsilon_greedy(Q, s, n_actions, eps):\n",
        "    if np.random.rand() < eps:\n",
        "        return np.random.randint(n_actions)\n",
        "    # tie-breaking: choose randomly among max actions\n",
        "    qs = Q[s]\n",
        "    return np.random.choice(np.flatnonzero(qs == qs.max()))\n",
        "\n",
        "def smooth(x, k=50):\n",
        "    if k <= 1: return x\n",
        "    out = []\n",
        "    csum = 0.0\n",
        "    buf = deque(maxlen=k)\n",
        "    for v in x:\n",
        "        buf.append(v)\n",
        "        csum = sum(buf)\n",
        "        out.append(csum / len(buf))\n",
        "    return out\n",
        "\n",
        "# -------------------- Core algorithms --------------------\n",
        "def sarsa_n_step(env, n_step=5, episodes=5000, alpha=0.1, gamma=0.99,\n",
        "                 eps_start=0.1, eps_end=0.02, eps_decay=0.999):\n",
        "    nS = env.observation_space.n\n",
        "    nA = env.action_space.n\n",
        "    Q = np.zeros((nS, nA))\n",
        "    returns = []\n",
        "\n",
        "    eps = eps_start\n",
        "    for ep in range(episodes):\n",
        "        state, _ = env.reset()\n",
        "        action = epsilon_greedy(Q, state, nA, eps)\n",
        "\n",
        "        states, actions, rewards = [], [], []\n",
        "        T = math.inf\n",
        "        t = 0\n",
        "\n",
        "        while True:\n",
        "            if t < T:\n",
        "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "                done = terminated or truncated\n",
        "\n",
        "                states.append(state)\n",
        "                actions.append(action)\n",
        "                rewards.append(reward)\n",
        "\n",
        "                if done:\n",
        "                    T = t + 1\n",
        "                else:\n",
        "                    next_action = epsilon_greedy(Q, next_state, nA, eps)\n",
        "                    state = next_state\n",
        "                    action = next_action\n",
        "\n",
        "            tau = t - n_step + 1\n",
        "            if tau >= 0:\n",
        "                # n-step return\n",
        "                G = 0.0\n",
        "                # sum of discounted rewards\n",
        "                for i in range(tau, min(tau + n_step, T)):\n",
        "                    G += (gamma ** (i - tau)) * rewards[i]\n",
        "                # bootstrap if episode not ended before tau + n\n",
        "                if tau + n_step < T and (tau + n_step) < len(states):\n",
        "                    s_tau_n = states[tau + n_step]\n",
        "                    a_tau_n = actions[tau + n_step]\n",
        "                    G += (gamma ** n_step) * Q[s_tau_n, a_tau_n]\n",
        "\n",
        "                s_tau = states[tau]\n",
        "                a_tau = actions[tau]\n",
        "                Q[s_tau, a_tau] += alpha * (G - Q[s_tau, a_tau])\n",
        "\n",
        "            if tau == T - 1:\n",
        "                break\n",
        "            t += 1\n",
        "\n",
        "        returns.append(sum(rewards))\n",
        "        eps = max(eps_end, eps * eps_decay)  # gentle decay\n",
        "\n",
        "    return Q, returns\n",
        "\n",
        "def sarsa_1_step(env, episodes=5000, alpha=0.1, gamma=0.99,\n",
        "                 eps_start=0.1, eps_end=0.02, eps_decay=0.999):\n",
        "    nS = env.observation_space.n\n",
        "    nA = env.action_space.n\n",
        "    Q = np.zeros((nS, nA))\n",
        "    returns = []\n",
        "\n",
        "    eps = eps_start\n",
        "    for ep in range(episodes):\n",
        "        s, _ = env.reset()\n",
        "        a = epsilon_greedy(Q, s, nA, eps)\n",
        "        done = False\n",
        "        ep_ret = 0.0\n",
        "\n",
        "        while not done:\n",
        "            s2, r, terminated, truncated, _ = env.step(a)\n",
        "            done = terminated or truncated\n",
        "            ep_ret += r\n",
        "            if not done:\n",
        "                a2 = epsilon_greedy(Q, s2, nA, eps)\n",
        "                td_target = r + gamma * Q[s2, a2]\n",
        "            else:\n",
        "                a2 = None\n",
        "                td_target = r\n",
        "            Q[s, a] += alpha * (td_target - Q[s, a])\n",
        "            s, a = s2, (a2 if a2 is not None else 0)\n",
        "\n",
        "        returns.append(ep_ret)\n",
        "        eps = max(eps_end, eps * eps_decay)\n",
        "\n",
        "    return Q, returns\n",
        "\n",
        "# -------------------- Run --------------------\n",
        "env = gym.make(\"CliffWalking-v1\")\n",
        "set_seed(env, seed=0)\n",
        "\n",
        "EPISODES = 5000\n",
        "ALPHA = 0.1\n",
        "GAMMA = 0.99\n",
        "EPS0, EPSF, EDECAY = 0.1, 0.02, 0.999\n",
        "\n",
        "Q5, ret5 = sarsa_n_step(env, n_step=5, episodes=EPISODES, alpha=ALPHA, gamma=GAMMA,\n",
        "                        eps_start=EPS0, eps_end=EPSF, eps_decay=EDECAY)\n",
        "\n",
        "# fresh env for fair comparison\n",
        "env2 = gym.make(\"CliffWalking-v1\")\n",
        "set_seed(env2, seed=0)\n",
        "Q1, ret1 = sarsa_1_step(env2, episodes=EPISODES, alpha=ALPHA, gamma=GAMMA,\n",
        "                        eps_start=EPS0, eps_end=EPSF, eps_decay=EDECAY)\n",
        "\n",
        "# -------------------- Plot --------------------\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.plot(ret1, label=\"SARSA (1-step)\")\n",
        "plt.plot(ret5, label=\"SARSA (n=5)\")\n",
        "plt.plot(smooth(ret1, 100), linestyle=\"--\", label=\"1-step (avg100)\")\n",
        "plt.plot(smooth(ret5, 100), linestyle=\"--\", label=\"n=5 (avg100)\")\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Return\")\n",
        "plt.title(\"CliffWalking: SARSA 1-step vs TD(5)\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "So6pIJ09Az4I",
        "outputId": "338a98ef-8bdc-426a-fb4a-423613372d9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "list index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1993115188.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0mEPS0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPSF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEDECAY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.02\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.999\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m Q5, ret5 = sarsa_n_step(env, n_step=5, episodes=EPISODES, alpha=ALPHA, gamma=GAMMA,\n\u001b[0m\u001b[1;32m    141\u001b[0m                         eps_start=EPS0, eps_end=EPSF, eps_decay=EDECAY)\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1993115188.py\u001b[0m in \u001b[0;36msarsa_n_step\u001b[0;34m(env, n_step, episodes, alpha, gamma, eps_start, eps_end, eps_decay)\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;31m# bootstrap if episode not ended before tau + n\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtau\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn_step\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                     \u001b[0ms_tau_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtau\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m                     \u001b[0ma_tau_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtau\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                     \u001b[0mG\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mn_step\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms_tau_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_tau_n\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2: Dyna-Q for CliffWalking\n",
        "\n",
        "**Objective**  \n",
        "Implement **Dyna-Q** on **CliffWalking-v1** and compare its learning performance to **SARSA (1-step)** and **SARSA TD(5)**. You will analyze sample efficiency, stability near the cliff, and sensitivity to planning steps.\n",
        "\n",
        "---\n",
        "\n",
        "### Environment\n",
        "- Use `CliffWalking-v1`\n",
        "---\n",
        "\n",
        "### Part A â Dyna-Q (Implementation)\n",
        "1. **Q-table**: maintain `Q[s, a]` (tabular).\n",
        "2. **Model**: learn an empirical model from experience.\n",
        "   - For each observed transition `(s, a, r, s')`, update a dictionary:\n",
        "     - Minimal: store the most recent `(s', r)` for `(s, a)`, **or**\n",
        "     - Advanced: store a **multiset** of outcomes for `(s, a)` with counts (to sample stochastically).\n",
        "3. **Real update (Q-learning)** after each env step:\n",
        "   Q(s,a) â Q(s,a) + Î± * (r + Î³ * max_a' Q(s',a') - Q(s,a))\n",
        "4. **Planning updates**: after each real step, perform `N` simulated updates:\n",
        "   - Sample a previously seen `(s_p, a_p)` from the model.\n",
        "   - Sample `(r_p, s'_p)` from that entry.\n",
        "   - Apply the same Q-learning backup using `(s_p, a_p, r_p, s'_p)`.\n",
        "5. Use epsilon-greedy exploration.\n",
        "\n",
        "---\n",
        "\n",
        "### Part B â Baselines (Re-use / Implement)\n",
        "- **SARSA (1-step)** with Îµ-greedy:\n",
        "  \\[\n",
        "  Q(s,a) \\leftarrow Q(s,a) + \\alpha\\big[r + \\gamma Q(s',a') - Q(s,a)\\big]\n",
        "  \\]\n",
        "- **SARSA TD(5)** (n-step SARSA with \\(n=5\\)), as in Exercise 1.\n",
        "\n",
        "Use the **same** Î³, Î±, Îµ schedule, and number of episodes for a fair comparison.\n",
        "\n",
        "---\n",
        "\n",
        "### Part C â Experiments & Comparisons\n",
        "1. **Learning curves**: plot **episode index vs. episode return** for:\n",
        "   - Dyna-Q with \\(N \\in \\{5, 20, 50\\}\\)\n",
        "   - SARSA (1-step)\n",
        "   - SARSA TD(5)\n",
        "2. **Sample efficiency**: report the **episode number** at which the average return over a sliding window (e.g., 100 episodes) first exceeds a chosen threshold (e.g., â30).\n",
        "3. **Stability near the cliff**: qualitatively inspect trajectories/policies; does the method hug the cliff or leave a safer margin?\n",
        "4. **Sensitivity to planning steps**: compare Dyna-Q across N; discuss diminishing returns vs. computation.\n",
        "5. **Statistical robustness**: run **â¥5 seeds**; plot mean Â± std (shaded) or report mean Â± std of final returns.\n",
        "\n",
        "---\n",
        "\n",
        "### Deliverables\n",
        "- **Code**: A driver script/notebook that reproduces your plots\n",
        "- **Plots** (embedded in the notebook):\n",
        "  - Learning curves (mean Â± std across seeds)\n",
        "  - Optional: heatmap of greedy policy/actions on the grid\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "h8NKZuvP5GZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BhimOYaY0zZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 3: Solve FrozenLake with Q-Learning and Dyna-Q (Stochastic Model)\n",
        "\n",
        "**Objective**  \n",
        "Implement and compare **Q-learning** and **Dyna-Q** on Gymnasiumâs `FrozenLake-v1`.  \n",
        "For Dyna-Q, your learned **transition model must handle multiple possible next states** per `(s, a)` (stochastic slip), i.e., store and sample **a distribution** over `(s', r)` outcomes rather than a single next state.\n",
        "\n",
        "---\n",
        "\n",
        "### Environment\n",
        "- Use `FrozenLake-v1` from `gymnasium.envs.toy_text`.\n",
        "- You can start with map 4Ã4; and then work with 8Ã8.\n",
        "- Start â Goal with slippery transitions (stochastic).  \n",
        "- Rewards: `+1` at goal, `0` otherwise (holes terminate with 0).\n",
        "\n",
        "---\n",
        "\n",
        "### Part A â Q-learning (baseline)\n",
        "1. Maintain a tabular action-value function `Q[s, a]`.\n",
        "2. Behavior: Îµ-greedy over `Q`.\n",
        "3. Update after each real step:\n",
        "   - target = r + Î³ * max_a' Q[s', a']   (if terminal: target = r)\n",
        "   - Q[s, a] â Q[s, a] + Î± * (target â Q[s, a])\n",
        "4. Train for several thousand episodes (e.g., 5,000) with an Îµ schedule (e.g., 0.2 â 0.01).\n",
        "\n",
        "---\n",
        "\n",
        "### Part B â Dyna-Q with a **stochastic transition model**\n",
        "1. **Empirical model (multinomial):** for each `(s, a)`, maintain a multiset of observed outcomes:\n",
        "   - `model[(s, a)] = [(s'_1, r_1, count_1), (s'_2, r_2, count_2), ...]`\n",
        "   - Update counts whenever you observe `(s, a, r, s')`.\n",
        "2. **Real step update (Q-learning):** same as Part A.\n",
        "3. **Planning steps (N per real step):**\n",
        "   - Sample a previously seen `(s_p, a_p)` uniformly (or with priority).\n",
        "   - Sample `(s'_p, r_p)` **from the empirical distribution** for `(s_p, a_p)` using counts as probabilities.\n",
        "   - Apply the same Q-learning backup with `(s_p, a_p, r_p, s'_p)`.\n",
        "4. Train with the same Îµ schedule and number of episodes; vary `N â {5, 20, 50}`.\n",
        "\n",
        "---\n",
        "\n",
        "### Experiments & Analysis\n",
        "1. **Learning curves:** plot episode index vs episode return (smoothed) for:\n",
        "   - Q-learning\n",
        "   - Dyna-Q (N=5, 20, 50)\n",
        "2. **Sample efficiency:** report the episode at which the moving-average return (e.g., window 100) first exceeds a threshold (you choose a reasonable value).\n",
        "3. **Effect of stochastic modeling:** briefly explain why storing a distribution over `(s', r)` matters on FrozenLake (slip), and what happens if you store only the most recent outcome.\n",
        "4. **Robustness:** run â¥5 random seeds; report mean Â± std of final evaluation returns.\n",
        "\n",
        "---\n",
        "\n",
        "### Deliverables\n",
        "- **Code** for Q-learning and Dyna-Q (with stochastic model).  \n",
        "- **Plots** of learning curves (include legend and axis labels).  \n",
        "- ** Discussion:** why Dyna-Q helps here; impact of N; importance of modeling multiple next states.\n",
        "\n",
        "---\n",
        "\n",
        "### Hints\n",
        "- For terminal transitions (goal/hole), the Q-learning target is simply `target = r` (no bootstrap).  \n",
        "- When sampling from the model, use probabilities `p_i = count_i / sum_j count_j`.  \n",
        "- Tie-break greedy action selection uniformly among argmax actions to avoid bias.  \n",
        "- Keep evaluation **greedy (Îµ=0)** and consistent across methods (same seeds and episode counts).\n"
      ],
      "metadata": {
        "id": "E4iLQFaGzJLj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d7FHlfk700lr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}