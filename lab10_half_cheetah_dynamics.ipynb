{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "199af124",
      "metadata": {
        "id": "199af124"
      },
      "source": [
        "\n",
        "# HalfCheetah: Learn a Dynamics Model from Random Rollouts (Then Validate It)\n",
        "\n",
        "**Goal:** In this notebook you'll (1) collect random experience tuples \\((s_t, a_t, r_t, s_{t+1})\\) from `HalfCheetah-v4`, (2) train a neural network to predict **state deltas** \\(\\Delta s = s_{t+1}-s_t\\), and (3) **validate** the model with one-step and multi-step (open-loop) rollouts.\n",
        "\n",
        "This mirrors the first phase of model-based control (e.g., MPPI): learn a model offline, then use it for planning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b3392de",
      "metadata": {
        "id": "2b3392de"
      },
      "source": [
        "\n",
        "## 0. Requirements\n",
        "\n",
        "- Python 3.9+\n",
        "- PyTorch `>= 1.10`\n",
        "- Gymnasium `>= 0.29`\n",
        "- MuJoCo with `HalfCheetah-v4` (install `mujoco` and `gymnasium[mujoco]`)\n",
        "\n",
        "```bash\n",
        "pip install \"gymnasium[mujoco]\" mujoco torch matplotlib\n",
        "```\n",
        "\n",
        "**Try and understand what RunningNormalizer does.**\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install \"gymnasium[mujoco]\" mujoco torch matplotlib"
      ],
      "metadata": {
        "id": "8H9anBJd8HSJ",
        "outputId": "bd879fe9-a072-4469-e296-d11057bd55e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "8H9anBJd8HSJ",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mujoco\n",
            "  Downloading mujoco-3.3.7-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (41 kB)\n",
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/42.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: gymnasium[mujoco] in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[mujoco]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[mujoco]) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[mujoco]) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium[mujoco]) (0.0.4)\n",
            "Requirement already satisfied: imageio>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium[mujoco]) (2.37.2)\n",
            "Requirement already satisfied: packaging>=23.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[mujoco]) (25.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from mujoco) (1.4.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.12/dist-packages (from mujoco) (1.13.0)\n",
            "Collecting glfw (from mujoco)\n",
            "  Downloading glfw-2.10.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.12/dist-packages (from mujoco) (3.1.10)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.12/dist-packages (from etils[epath]->mujoco) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.12/dist-packages (from etils[epath]->mujoco) (3.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Downloading mujoco-3.3.7-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m87.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading glfw-2.10.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl (243 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m243.5/243.5 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: glfw, mujoco\n",
            "Successfully installed glfw-2.10.0 mujoco-3.3.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c1213bd2",
      "metadata": {
        "id": "c1213bd2"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os, math, random, time\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple\n",
        "import numpy as np\n",
        "import torch as th\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import gymnasium as gym\n",
        "\n",
        "def to_t(x):\n",
        "    return th.as_tensor(x, dtype=th.float32)\n",
        "\n",
        "def fanin_init(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        bound = 1.0 / math.sqrt(m.weight.size(1))\n",
        "        nn.init.uniform_(m.weight, -bound, +bound)\n",
        "        nn.init.zeros_(m.bias)\n",
        "\n",
        "class RunningNormalizer:\n",
        "    \"\"\"Feature-wise running mean/std (Welford).\"\"\"\n",
        "    def __init__(self, dim, eps=1e-8):\n",
        "        self.dim = dim\n",
        "        self.count = 0\n",
        "        self.mean = np.zeros(dim, dtype=np.float64)\n",
        "        self.M2   = np.zeros(dim, dtype=np.float64)\n",
        "        self.eps  = eps\n",
        "\n",
        "    def update(self, x: np.ndarray):\n",
        "        x = np.asarray(x)\n",
        "        if x.ndim == 1: x = x[None, :]\n",
        "        for v in x:\n",
        "            self.count += 1\n",
        "            d = v - self.mean\n",
        "            self.mean += d / self.count\n",
        "            d2 = v - self.mean\n",
        "            self.M2 += d * d2\n",
        "\n",
        "    @property\n",
        "    def var(self):\n",
        "        if self.count < 2: return np.ones(self.dim, dtype=np.float64)\n",
        "        return self.M2 / (self.count - 1 + 1e-12)\n",
        "\n",
        "    @property\n",
        "    def std(self):\n",
        "        return np.sqrt(self.var + self.eps)\n",
        "\n",
        "    def normalize(self, x): return (x - self.mean) / self.std\n",
        "    def denormalize(self, x): return x * self.std + self.mean\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed); np.random.seed(seed); th.manual_seed(seed)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "304b5ce5",
      "metadata": {
        "id": "304b5ce5"
      },
      "source": [
        "\n",
        "## Initializing Environment and Figure Out Observation Structure\n",
        "\n",
        "`HalfCheetah-v4` exposes observations as `[qpos[1:], qvel[:]]`. The forward velocity is `qvel[0]`, which sits at index `len(qpos[1:])` inside the observation vector. We'll extract that index for later validation/plots. :::: This is important for planning, if we want to know what each state represents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "5de69613",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5de69613",
        "outputId": "74365f6d-ba91-4a69-f24c-e2dda9328179"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "obs_dim: 17 act_dim: 6 qvel_start: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gymnasium/envs/registration.py:512: DeprecationWarning: \u001b[33mWARN: The environment HalfCheetah-v4 is out of date. You should consider upgrading to version `v5`.\u001b[0m\n",
            "  logger.deprecation(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "set_seed(42)\n",
        "env = gym.make(\"HalfCheetah-v4\")\n",
        "obs_dim = env.observation_space.shape[0]\n",
        "act_dim = env.action_space.shape[0]\n",
        "act_low = env.action_space.low\n",
        "act_high = env.action_space.high\n",
        "\n",
        "# Find start index of qvel inside obs = [qpos[1:], qvel[:]]\n",
        "nq = env.unwrapped.model.nq\n",
        "qvel_start = int(nq - 1)\n",
        "print(\"obs_dim:\", obs_dim, \"act_dim:\", act_dim, \"qvel_start:\", qvel_start)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e071b0bb",
      "metadata": {
        "id": "e071b0bb"
      },
      "source": [
        "## üß© Task 1: Prepare the Replay Buffer\n",
        "**Goal:** Store transitions \\((s_t, a_t, s_{t+1})\\) and return training pairs \\((x, y) = (s_t, a_t, s_{t+1} - s_t)\\).\n",
        "\n",
        "**Instructions:**\n",
        "- Implement `add()` to record transitions.\n",
        "- Add a `sample()` method to randomly sample batch of certain size.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "08628879",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08628879",
        "outputId": "fd907c2c-817e-4e9f-9c49-533d2fa03463"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Replay buffer created.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "class Replay:\n",
        "    def __init__(self, obs_dim, act_dim, size=100000):\n",
        "        self.obs_buf = np.zeros((size, obs_dim), dtype=np.float32)\n",
        "        self.act_buf = np.zeros((size, act_dim), dtype=np.float32)\n",
        "        self.rew_buf = np.zeros(size, dtype=np.float32)\n",
        "        self.next_obs_buf = np.zeros((size, obs_dim), dtype=np.float32)\n",
        "        self.done_buf = np.zeros(size, dtype=np.float32)\n",
        "        self.ptr = 0\n",
        "        self.size = size\n",
        "        self.full = False\n",
        "\n",
        "    def store(self, obs, act, rew, next_obs, done):\n",
        "        i = self.ptr\n",
        "        self.obs_buf[i] = obs\n",
        "        self.act_buf[i] = act\n",
        "        self.rew_buf[i] = rew\n",
        "        self.next_obs_buf[i] = next_obs\n",
        "        self.done_buf[i] = done\n",
        "\n",
        "        self.ptr = (self.ptr + 1) % self.size\n",
        "        if self.ptr == 0:\n",
        "            self.full = True\n",
        "\n",
        "    def sample(self, batch):\n",
        "        max_size = self.size if self.full else self.ptr\n",
        "        idx = np.random.randint(0, max_size, size=batch)\n",
        "        return (self.obs_buf[idx],\n",
        "                self.act_buf[idx],\n",
        "                self.rew_buf[idx],\n",
        "                self.next_obs_buf[idx],\n",
        "                self.done_buf[idx])\n",
        "\n",
        "replay = Replay(obs_dim, act_dim)\n",
        "print(\"Replay buffer created.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "374692ef",
      "metadata": {
        "id": "374692ef"
      },
      "source": [
        "\n",
        "## Task 2. Collect Random Rollouts\n",
        "\n",
        "- Gather random actions for a number of steps to create our training dataset. Collect data for 100000 steps.\n",
        "- Call the function and fill the replay buffer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "e2d7b840",
      "metadata": {
        "id": "e2d7b840"
      },
      "outputs": [],
      "source": [
        "def collect_random(env, replay, steps=10000, seed=42):\n",
        "    np.random.seed(seed)\n",
        "    obs, _ = env.reset(seed=seed)\n",
        "    for _ in range(steps):\n",
        "        act = env.action_space.sample()\n",
        "        next_obs, rew, done, trunc, _ = env.step(act)\n",
        "        replay.store(obs, act, rew, next_obs, done)\n",
        "\n",
        "        if done or trunc:\n",
        "            obs, _ = env.reset()\n",
        "        else:\n",
        "            obs = next_obs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7689c7f",
      "metadata": {
        "id": "c7689c7f"
      },
      "source": [
        "\n",
        "## Task 3. Update normalizers from the collected random data in the replay buffer\n",
        "\n",
        "We normalize inputs (`[s,a]`) and targets (`Œîs = s' - s`) for stable training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "39332ea3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39332ea3",
        "outputId": "32719eeb-50e2-40fc-f2a0-a3d349c3d89e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalizers ready.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "obs_norm = RunningNormalizer(obs_dim)\n",
        "inp_norm = RunningNormalizer(obs_dim + act_dim)\n",
        "targ_norm = RunningNormalizer(obs_dim)\n",
        "\n",
        "# write the function to update the normalizers from the data collected in the buffer\n",
        "def update_normalizers_from_buffer(replay):\n",
        "    max_size = replay.size if replay.full else replay.ptr\n",
        "\n",
        "    for i in range(max_size):\n",
        "        s = replay.obs_buf[i]\n",
        "        a = replay.act_buf[i]\n",
        "        s_next = replay.next_obs_buf[i]\n",
        "\n",
        "        inp = np.concatenate([s, a], axis=-1)\n",
        "        delta_s = s_next - s\n",
        "\n",
        "        obs_norm.update(s)\n",
        "        inp_norm.update(inp)\n",
        "        targ_norm.update(delta_s)\n",
        "\n",
        "\n",
        "update_normalizers_from_buffer(replay)\n",
        "print(\"Normalizers ready.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "566164a4",
      "metadata": {
        "id": "566164a4"
      },
      "source": [
        "\n",
        "## Defining the Neural Dynamics Model\n",
        "\n",
        "We predict **normalized** `Œîs` from **normalized** `[s, a]`.\n",
        "NN parameters:\n",
        "\n",
        "- initialize a deterministic NN with a ExponentialLR sceduler( that decays the learning rate with epoch)\n",
        "- width = 200, depth = 3, lr = 1e-3, weight_decay - 1e-5, gamma for scheduler = 0.8\n",
        "- These are a starting point but not the best parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "a6d3eda1",
      "metadata": {
        "id": "a6d3eda1"
      },
      "outputs": [],
      "source": [
        "\n",
        "class DetMLP(nn.Module):\n",
        "    \"\"\"Predicts Œîstate deterministically.\"\"\"\n",
        "    def __init__(self, in_dim, out_dim, width=200, depth=3):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        last = in_dim\n",
        "        for _ in range(depth):\n",
        "            layers += [nn.Linear(last, width), nn.ReLU()]\n",
        "            last = width\n",
        "        layers += [nn.Linear(last, out_dim)]\n",
        "        self.net = nn.Sequential(*layers)\n",
        "        self.apply(fanin_init)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "in_dim = obs_dim + act_dim\n",
        "out_dim = obs_dim\n",
        "model = DetMLP(in_dim, out_dim, width=200, depth=3)\n",
        "opt = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
        "scheduler = optim.lr_scheduler.ExponentialLR(opt, gamma=0.8)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5a3db88",
      "metadata": {
        "id": "a5a3db88"
      },
      "source": [
        "\n",
        "## Task 4. Train the Model\n",
        "\n",
        "We minimize MSE between predicted normalized `Œîs` and target normalized `Œîs`.\n",
        "\n",
        "- Train in batches, keep the batch size 256\n",
        "- Use a learning rate scheduler that decays the learning rate as training progresses. You may use the pytorch utility. See how the learning rate decays with each epoch.\n",
        "- Train for 30 epochs and plot the training curve. Loss vs epoch.\n",
        "- Find the best parameters(defined in the previous block)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "replay = Replay(obs_dim, act_dim)\n",
        "\n",
        "collect_random(env, replay, steps=10_000)  # Ëá≥Â∞ëÂá†ÂçÉÊ≠•\n",
        "print(\"ptr =\", replay.ptr)  # Á°ÆËÆ§ > 0\n"
      ],
      "metadata": {
        "id": "LcqtjpIRAqz2",
        "outputId": "d39a49b4-399b-4ae3-8abd-a3248f5d57c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "LcqtjpIRAqz2",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ptr = 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "obs_norm = RunningNormalizer(obs_dim)\n",
        "inp_norm = RunningNormalizer(obs_dim + act_dim)\n",
        "tgt_norm = RunningNormalizer(obs_dim)\n",
        "update_normalizers_from_buffer(replay)\n"
      ],
      "metadata": {
        "id": "kBOAPzorBCPg"
      },
      "id": "kBOAPzorBCPg",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "4a39c80c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "4a39c80c",
        "outputId": "e988a82e-d97b-4e15-de2a-759c12620791"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1377349838.py:48: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  def normalize(self, x): return (x - self.mean) / self.std\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 must have the same dtype, but got Double and Float",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1838966351.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1838966351.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, replay, epochs, batch_size)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mtgt_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarg_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3931823703.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0min_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs_dim\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mact_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype, but got Double and Float"
          ]
        }
      ],
      "source": [
        "\n",
        "def train_model(model, replay, epochs=30, batch_size=None):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "    mse = nn.MSELoss()\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        losses = []\n",
        "        for _ in range(100):  # 100 gradient steps per epoch\n",
        "            s, a, _, s_next, _ = replay.sample(batch_size)\n",
        "            s = th.tensor(s, dtype=th.float32)\n",
        "            a = th.tensor(a, dtype=th.float32)\n",
        "            s_next = th.tensor(s_next, dtype=th.float32)\n",
        "\n",
        "            delta_s = s_next - s\n",
        "\n",
        "            inp = th.cat([s, a], dim=-1)\n",
        "\n",
        "            # normalize\n",
        "            inp_n = inp_norm.normalize(inp)\n",
        "            tgt_n = targ_norm.normalize(delta_s)\n",
        "\n",
        "            pred = model(inp_n)\n",
        "\n",
        "            loss = mse(pred, tgt_n)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            losses.append(loss.item())\n",
        "\n",
        "        print(f\"Epoch {ep}: loss={sum(losses)/len(losses):.4f}\")\n",
        "\n",
        "\n",
        "losses = train_model(model, replay, epochs=10, batch_size=64)\n",
        "plt.figure()\n",
        "plt.plot(losses)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"MSE (normalized Œîs)\")\n",
        "plt.title(\"Model Training Loss\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34a1c7c0",
      "metadata": {
        "id": "34a1c7c0"
      },
      "source": [
        "## Task 5. Validate your model: One-Step and Multi-Step Prediction Error\n",
        "\n",
        "- Evaluate your trained model on a held-out set of random transitions.\n",
        "Generate a batch of unseen samples, predict the next-state delta, and compute the one-step MSE.\n",
        "\n",
        "- Repeat with open-loop rollouts of length k.\n",
        "Drive both the real environment and the model with the same action sequence, then report how prediction error grows across steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1efe4271",
      "metadata": {
        "id": "1efe4271"
      },
      "outputs": [],
      "source": [
        "\n",
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c5a173d",
      "metadata": {
        "id": "5c5a173d"
      },
      "source": [
        "\n",
        "## Task 6. Visualize Rollout Trajectories\n",
        "\n",
        "**Setup**\n",
        "Call model.eval() so gradients stay off.\n",
        "Reset the env with the provided seed; keep a copy of the initial observation.\n",
        "\n",
        "\n",
        "**Choose actions**\n",
        "Pre-sample k actions from env.action_space.sample() so the real system and the model rollout see the same sequence.\n",
        "\n",
        "**Roll forward**\n",
        "For each action:\n",
        "Step the real env (env.step(a)), append the new observation.\n",
        "For the model path:\n",
        "Build [s_model, a], normalize via inp_norm.normalize, turn into a tensor with to_t.\n",
        "Run the network, de-normalize with targ_norm.denormalize, add to the last model state, append.\n",
        "Stop early if the env terminates or truncates.\n",
        "\n",
        "**Plot**\n",
        "Plot the real trajectory as one line, model trajectory as another.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aebb7a76",
      "metadata": {
        "id": "aebb7a76"
      },
      "outputs": [],
      "source": [
        "\n",
        "def visualize_rollout(env, model, k=50, dims=(0, 5, 10), seed=2025):\n",
        "    pass\n",
        "\n",
        "# Uncomment to visualize\n",
        "dims = list(range(1, 17))\n",
        "# visualize_rollout(env, model, k=50, dims=dims)#(qvel_start, qvel_start+1, qvel_start+2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c34f48f2",
      "metadata": {
        "id": "c34f48f2"
      },
      "source": [
        "\n",
        "## 9. Answer the questions :\n",
        "\n",
        "1. How good is your model?\n",
        "2. Is this training enough for planning, or do we need continual training?\n",
        "3. How is this system different from the mountain car problem? Why can't we learn this in one episode?\n",
        "4. Why do we use a runningnormalizer instead of a static normalizer? Think about the nature of the algorithm taught in class.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mquad_cuda",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}